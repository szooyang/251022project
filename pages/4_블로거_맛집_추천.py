# pages/4_ë¸”ë¡œê±°_ë§›ì§‘_ì¶”ì²œ.py
# ë¸”ë¡œê±° ë§›ì§‘ ì¶”ì²œ â€” OpenAI í‚¤ë§Œìœ¼ë¡œ 'ì‹¤ê²€ìƒ‰ ê¸°ë°˜' ì¶”ì²œ
# (ë„¤ì´ë²„ VIEW/ì›¹ ê²€ìƒ‰ì„ íŒŒì‹±í•´ í›„ë³´ë¥¼ ë§Œë“¤ê³ , OpenAIëŠ” ë¦¬ë­í¬/ìš”ì•½ë§Œ ìˆ˜í–‰)

import re
import json
import difflib
import requests
import pandas as pd
import streamlit as st
from urllib.parse import quote_plus, urlparse

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë¸”ë¡œê±° ë§›ì§‘ ì¶”ì²œ", page_icon="ğŸ“", layout="wide")
st.title("ğŸ“ ë¸”ë¡œê±° ë§›ì§‘ ì¶”ì²œ â€” OpenAI í‚¤ë§Œìœ¼ë¡œ â€˜ì‹¤ê²€ìƒ‰ ê¸°ë°˜â€™")
st.caption(
    "ë„¤ì´ë²„ VIEW(ë¸”ë¡œê·¸/ì¹´í˜) ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ë³ê²Œ íŒŒì‹±í•´ ì‹¤ì œ ë§í¬ê°€ ìˆëŠ” í›„ë³´ë§Œ ëª¨ìœ¼ê³ , "
    "OpenAIëŠ” ë¦¬ë­í¬/ìš”ì•½ë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤. (íƒ€ì‚¬ API ë¶ˆí•„ìš”)"
)

# OpenAI ì¤€ë¹„ (Secretsì—ì„œë§Œ ì½ìŒ)
OPENAI_KEY = st.secrets.get("OPENAI_API_KEY", "")
if not OPENAI_KEY:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. Streamlit Secretsì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

try:
    from openai import OpenAI
except Exception:
    st.error("openai íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— openai>=1.30 ì¶”ê°€ í›„ ì¬ë°°í¬í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=OPENAI_KEY)

# ì…ë ¥ UI
default_food = st.session_state.get("selected_food", "")
SEOUL_GU = [
    "ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ë…¸ì›êµ¬", "ë„ë´‰êµ¬",
    "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬", "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬",
    "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"
]

c1, c2, c3 = st.columns([1, 1, 1])
with c1:
    gu = st.selectbox("ì„œìš¸ êµ¬", SEOUL_GU, index=SEOUL_GU.index("ê°•ë‚¨êµ¬"), key="blog_gu")
with c2:
    food = st.text_input("ìŒì‹/í‚¤ì›Œë“œ", value=str(default_food) or "ë¹„ë¹”ë°¥", key="blog_food")
with c3:
    topk = st.slider("ìµœëŒ€ ì¶”ì²œ ìˆ˜", min_value=3, max_value=10, value=5, step=1, key="blog_topk")

st.write("---")
go = st.button("ë¸”ë¡œê·¸ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œ ë°›ê¸° ğŸš€", use_container_width=True, key="blog_go")

# ë„¤ì´ë²„ VIEW/WEB íŒŒì„œ(ë¬´í‚¤)
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/130.0.0.0 Safari/537.36"
    )
}
BLOG_DOMAINS = ("blog.naver.com", "m.blog.naver.com", "tistory.com", "brunch.co.kr")

def fetch_naver_serp(mode: str, gu_name: str, food_kw: str):
    """
    mode: 'view' (ë¸”ë¡œê·¸/ì¹´í˜ ì¤‘ì‹¬) ë˜ëŠ” 'web' (ì¼ë°˜ ì›¹)
    """
    if mode == "view":
        q = f"{gu_name} {food_kw} ë§›ì§‘ í›„ê¸°"
        url = (
            "https://search.naver.com/search.naver"
            f"?where=view&sm=tab_jum&query={quote_plus(q)}"
        )
    else:
        q = f"{gu_name} {food_kw} ë§›ì§‘"
        url = (
            "https://search.naver.com/search.naver"
            f"?where=web&sm=tab_jum&query={quote_plus(q)}"
        )
    r = requests.get(url, headers=HEADERS, timeout=10)
    r.raise_for_status()
    return r.text, q

def extract_candidates(html: str, q: str, max_items: int = 24, prefer_blogs: bool = True):
    """
    HTMLì—ì„œ ì œëª©/ë§í¬ë¥¼ ì¶”ì¶œí•´ í›„ë³´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
    ë°˜í™˜: [{name, link, snippet, score}]
    - ë¸”ë¡œê·¸/í‹°ìŠ¤í† ë¦¬/ë¸ŒëŸ°ì¹˜ ë„ë©”ì¸ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    """
    # ëª¨ë“  a íƒœê·¸ rough ì¶”ì¶œ
    raw = re.findall(r'<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', html, flags=re.I | re.S)
    pairs = []
    for href, inner in raw:
        if not href.startswith("http"):
            continue
        title = re.sub("<.*?>", "", inner).strip()
        if not title or len(title) < 2:
            continue
        if any(bad in href for bad in ["login", "policy", "javascript:", "naversearchad"]):
            continue
        pairs.append((href, title))

    # ìœ ì‚¬ë„ ê¸°ë°˜ í›„ë³´í™”
    cands = []
    seen = set()
    q_low = q.lower()
    for href, title in pairs:
        t = title.replace("\n", " ").strip()
        sim = difflib.SequenceMatcher(None, t.lower(), q_low).ratio()

        bonus_kw = 0.05 if any(k in t for k in ["ë§›ì§‘", "ì‹ë‹¹", "í›„ê¸°", "ë¦¬ë·°"]) else 0.0
        host = urlparse(href).netloc.lower()
        bonus_blog = 0.10 if (prefer_blogs and any(d in host for d in BLOG_DOMAINS)) else 0.0

        score = sim + bonus_kw + bonus_blog
        key = (t, href)
        if key not in seen:
            seen.add(key)
            cands.append({"name": t, "link": href, "snippet": "", "score": score})

    # íƒ€ì´í‹€ ì£¼ë³€ í…ìŠ¤íŠ¸ë¥¼ ê°„ë‹¨ ìŠ¤ë‹ˆí«ìœ¼ë¡œ
    for c in cands:
        try:
            idx = html.find(c["name"])
            if idx != -1:
                start = max(0, idx - 100)
                end = min(len(html), idx + 200)
                snippet = re.sub("<.*?>", " ", html[start:end])
                snippet = re.sub(r"\s+", " ", snippet).strip()
                if len(snippet) > 160:
                    snippet = snippet[:160] + "..."
                c["snippet"] = snippet
        except Exception:
            pass

    cands.sort(key=lambda x: x["score"], reverse=True)
    return cands[:max_items]

# OpenAI: í›„ë³´ ë¦¬ë­í¬/ìš”ì•½(ìƒˆ ê°€ê²Œ ìƒì„± ê¸ˆì§€)
SYS = (
    "You are a strict re-ranker/summarizer. "
    "GIVEN ONLY the provided candidates from web search (with title/link/snippet), "
    "recommend top places. NEVER invent new restaurant names. "
    "Output compact JSON: {"
    "\"summary\":\"Korean one-line with emojis\", "
    "\"items\":[{\"name\":\"string\",\"reason\":\"Korean short\",\"link\":\"url\"}]"
    "} in Korean."
)

def ai_rerank(cands, topn: int):
    prompt = {
        "role": "user",
        "content": (
            "ì•„ë˜ëŠ” ë„¤ì´ë²„ VIEW/ë¸”ë¡œê·¸ ê²€ìƒ‰ì—ì„œ ì¶”ì¶œí•œ í›„ë³´ë“¤ì…ë‹ˆë‹¤. "
            f"ì¡°ê±´: ì„œìš¸ {gu}, í‚¤ì›Œë“œ: {food}. "
            f"ì´ í›„ë³´ë“¤ë§Œ ê°€ì§€ê³ , ìƒìœ„ {topn}ê°œë¥¼ ë½‘ì•„ ê°„ë‹¨ ì½”ë©˜íŠ¸ì™€ í•¨ê»˜ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”. "
            "ìƒˆë¡œìš´ ìƒí˜¸ëª…ì„ ë§Œë“¤ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n\n"
            + json.dumps(cands, ensure_ascii=False)
        ),
    }
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": SYS}, prompt],
        response_format={"type": "json_object"},
        temperature=0.2,
    )
    return json.loads(resp.choices[0].message.content)

# ì‹¤í–‰
if go:
    if not food.strip():
        st.warning("ìŒì‹/í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”!")
        st.stop()

    # 1) VIEW(ë¸”ë¡œê·¸/ì¹´í˜) ìš°ì„ 
    try:
        with st.spinner("ë„¤ì´ë²„ VIEW(ë¸”ë¡œê·¸/ì¹´í˜)ì—ì„œ í›„ë³´ ìˆ˜ì§‘ ì¤‘â€¦"):
            html_view, query = fetch_naver_serp("view", gu, food)
            candidates = extract_candidates(html_view, query, max_items=30, prefer_blogs=True)
    except Exception as e:
        st.info(f"VIEW ìˆ˜ì§‘ ì‹¤íŒ¨, ì¼ë°˜ ì›¹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤: {e}")
        candidates = []

    # 2) ë¶€ì¡±í•˜ë©´ WEB ë³´ê°•
    if not candidates:
        try:
            with st.spinner("ë„¤ì´ë²„ WEB(ì¼ë°˜)ì—ì„œ í›„ë³´ ìˆ˜ì§‘ ì¤‘â€¦"):
                html_web, query_web = fetch_naver_serp("web", gu, food)
                candidates = extract_candidates(html_web, query_web, max_items=30, prefer_blogs=False)
        except Exception as e:
            st.error(f"ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            st.stop()

    if not candidates:
        st.info("í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. í‚¤ì›Œë“œë¥¼ ë°”ê¾¸ê±°ë‚˜ ë” ì¼ë°˜ì ì¸ í‘œí˜„ìœ¼ë¡œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # 3) OpenAI ë¦¬ë­í¬/ìš”ì•½ (ìƒì„± ê¸ˆì§€)
    try:
        with st.spinner("AIê°€ ë¸”ë¡œê±° í›„ë³´ë“¤ë§Œ ê°€ì§€ê³  ì•ˆì „í•˜ê²Œ ë¦¬ë­í¬ ì¤‘â€¦"):
            ai = ai_rerank(
                [{"name": c["name"], "link": c["link"], "snippet": c["snippet"]} for c in candidates],
                topn=topk,
            )
    except Exception as e:
        st.error(f"AI ìš”ì•½ ì‹¤íŒ¨: {e}")
        st.stop()

    summary = ai.get("summary", "")
    items = ai.get("items", [])[:topk]

    st.success(f"{gu} Â· {food} â€” ë¸”ë¡œê±° ê¸°ë°˜ ì¶”ì²œ TOP{len(items)}")

    rows = []
    for it in items:
        name = it.get("name", "")
        reason = it.get("reason", "")
        link = it.get("link", "#")
        snip = ""
        for c in candidates:
            if c["name"] == name and c["link"] == link:
                snip = c.get("snippet", "")
                break

        st.markdown(f"**ğŸ½ï¸ {name}**\n- {reason}\n- ğŸ”— [ë§í¬]({link})")
        if snip:
            st.caption(snip)
        st.divider()

        rows.append({"ì´ë¦„": name, "ì½”ë©˜íŠ¸": reason, "ë§í¬": link, "ìŠ¤ë‹ˆí«": snip})

    df = pd.DataFrame(rows)
    df.index = range(1, len(df) + 1)
    st.dataframe(df, use_container_width=True)

    if summary:
        st.subheader("ğŸ¤– í•œ ì¤„ ìš”ì•½")
        st.markdown(summary)

    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv,
        file_name="blogger_based_recommendations.csv",
        mime="text/csv",
        use_container_width=True,
    )

else:
    st.info("êµ¬/í‚¤ì›Œë“œ/ê°œìˆ˜ ì •í•˜ê³  â€˜ë¸”ë¡œê·¸ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œ ë°›ê¸°â€™ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”. OpenAI í‚¤ë§Œìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
