# pages/5_ë¸”ë¡œê±°_ë§›ì§‘_ì¶”ì²œ.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¸”ë¡œê±° ë§›ì§‘ ì¶”ì²œ â€” OpenAI í‚¤ë§Œìœ¼ë¡œ 'ì‹¤ê²€ìƒ‰ ê¸°ë°˜' ì¶”ì²œ
# (ë„¤ì´ë²„ VIEW/ë¸”ë¡œê·¸ ê²°ê³¼ë¥¼ ìš°ì„  íŒŒì‹± â†’ í›„ë³´ë¥¼ OpenAIê°€ ë¦¬ë­í¬/ìš”ì•½)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import re, json, difflib
import requests
import pandas as pd
import streamlit as st
from urllib.parse import quote_plus, urlparse

# â”€â”€ í˜ì´ì§€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ë¸”ë¡œê±° ë§›ì§‘ ì¶”ì²œ", page_icon="ğŸ“", layout="wide")
st.title("ğŸ“ ë¸”ë¡œê±° ë§›ì§‘ ì¶”ì²œ â€” OpenAI í‚¤ë§Œìœ¼ë¡œ â€˜ì‹¤ê²€ìƒ‰ ê¸°ë°˜â€™")
st.caption("ë„¤ì´ë²„ VIEW(ë¸”ë¡œê·¸/ì¹´í˜) ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ë³ê²Œ íŒŒì‹±í•´ ì‹¤ì œ ë§í¬ê°€ ìˆëŠ” í›„ë³´ë§Œ ëª¨ìœ¼ê³ , OpenAIëŠ” ë¦¬ë­í¬/ìš”ì•½ë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤. (íƒ€ì‚¬ API ë¶ˆí•„ìš”)")

# â”€â”€ OpenAI ì¤€ë¹„ (Secretsì—ì„œë§Œ ì½ìŒ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_KEY = st.secrets.get("OPENAI_API_KEY", "")
if not OPENAI_KEY:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. Streamlit Secretsì— `OPENAI_API_KEY`ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

try:
    from openai import OpenAI
except Exception:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— `openai>=1.30` ì¶”ê°€ í›„ ì¬ë°°í¬í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=OPENAI_KEY)

# â”€â”€ ì…ë ¥ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
default_food = st.session_state.get("selected_food", "")
SEOUL_GU = [
    "ê°•ë‚¨êµ¬","ê°•ë™êµ¬","ê°•ë¶êµ¬","ê°•ì„œêµ¬","ê´€ì•…êµ¬","ê´‘ì§„êµ¬","êµ¬ë¡œêµ¬","ê¸ˆì²œêµ¬","ë…¸ì›êµ¬","ë„ë´‰êµ¬",
    "ë™ëŒ€ë¬¸êµ¬","ë™ì‘êµ¬","ë§ˆí¬êµ¬","ì„œëŒ€ë¬¸êµ¬","ì„œì´ˆêµ¬","ì„±ë™êµ¬","ì„±ë¶êµ¬","ì†¡íŒŒêµ¬","ì–‘ì²œêµ¬","ì˜ë“±í¬êµ¬",
    "ìš©ì‚°êµ¬","ì€í‰êµ¬","ì¢…ë¡œêµ¬","ì¤‘êµ¬","ì¤‘ë‘êµ¬"
]

c1, c2, c3 = st.columns([1, 1, 1])
with c1:
    gu = st.selectbox("ì„œìš¸ **êµ¬**", SEOUL_GU, index=SEOUL_GU.index("ê°•ë‚¨êµ¬"), key="blog_gu")
with c2:
    food = st.text_input("ìŒì‹/í‚¤ì›Œë“œ", value=str(default_food) or "ë¹„ë¹”ë°¥", key="blog_food")
with c3:
    topk = st.slider("ìµœëŒ€ ì¶”ì²œ ìˆ˜", 3, 10, 5, key="blog_topk")

st.write("---")
go = st.button("ë¸”ë¡œê·¸ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œ ë°›ê¸° ğŸš€", use_container_width=True, key="blog_go")

# â”€â”€ ë„¤ì´ë²„ VIEW/WEB íŒŒì„œ(ë¬´í‚¤) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/130.0.0.0 Safari/537.36"
    )
}
BLOG_DOMAINS = ("blog.naver.com", "m.blog.naver.com", "tistory.com", "brunch.co.kr")

def fetch_naver_serp(mode: str, gu: str, food: str):
    """
    mode: 'view' (ë¸”ë¡œê·¸/ì¹´í˜ ì¤‘ì‹¬) ë˜ëŠ” 'web' (ì¼ë°˜ ì›¹)
    """
    if mode == "view":
        q = f"{gu} {food} ë§›ì§‘ í›„ê¸°"
        url = f"https://search.naver.com/search.naver?where=view&sm=tab_jum&query={quote_plus(q)}"
    else:
        q = f"{gu} {food} ë§›ì§‘"
        url = f"https://search.naver.com/search.naver?where=web&sm=tab_jum&query={quote_plus(q)}"
    r = requests.get(url, headers=HEADERS, timeout=10)
    r.raise_for_status()
    return r.text, q

def extract_candidates(html: str, q: str, max_items: int = 24, prefer_blogs: bool = True):
    """
    HTMLì—ì„œ ì œëª©/ë§í¬ë¥¼ ìœ ì—°í•˜ê²Œ ì¶”ì¶œí•´ í›„ë³´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    ë°˜í™˜ ìš”ì†Œ: [{name, link, snippet, score}]
    - ë¸”ë¡œê·¸/í‹°ìŠ¤í† ë¦¬/ë¸ŒëŸ°ì¹˜ ë„ë©”ì¸ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    """
    # ëª¨ë“  a íƒœê·¸ rough ì¶”ì¶œ
    raw = re.findall(r'<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', html, flags=re.I | re.S)
    pairs = []
    for href, inner in raw:
        if not href.startswith("http"):
            continue
        title = re.sub("<.*?>", "", inner).strip()
        if not title or len(title) < 2:
            continue
        if any(bad in href for bad in ["login", "policy", "javascript:", "naversearchad"]):
            continue
        pairs.append((href, title))

    # ìœ ì‚¬ë„ ê¸°ë°˜ í›„ë³´í™”
    cands, seen = [], set()
    q_low = q.lower()
    for href, title in pairs:
        t = title.replace("\n", " ").strip()
        sim = difflib.SequenceMatcher(None, t.lower(), q_low).ratio()

        # í‚¤ì›Œë“œ/êµ¬/ë§›ì§‘ ê´€ë ¨ ê°€ì¤‘
        bonus_kw = 0.05 if any(k in t for k in ["ë§›ì§‘", "ì‹ë‹¹", "í›„ê¸°", "ë¦¬ë·°"]) else 0.0
        # ë¸”ë¡œê·¸ ë„ë©”ì¸ ê°€ì¤‘
        host = urlparse(href).netloc.lower()
        bonus_blog = 0.10 if (prefer_blogs and any(d in host for d in BLOG_DOMAINS)) else 0.0

        score = sim + bonus_kw + bonus_blog
        key = (t, href)
        if key not in seen:
            seen.add(key)
            cands.append({"name": t, "link": href, "snippet": "", "score": score})

    # íƒ€ì´í‹€ ì£¼ë³€ í…ìŠ¤íŠ¸ë¥¼ ê°„ë‹¨ ìŠ¤ë‹ˆí«ìœ¼ë¡œ
    for c in cands:
        try:
            idx = html.find(c["name"])
            if idx != -1:
                start = max(0, idx - 100)
                end = min(len(html), idx + 200)
                snippet = re.sub("<.*?>", " ", html[start:end])
                snippet = re.sub(r"\s+", " ", snippet).strip()
                c["snippet"] = (snippet[:160] + "...") if len(snippet) > 160 else snippet
        except Exception:
            pass

    cands.sort(key=lambda x: x["score"], reverse=True)
    return cands[:max_items]

# â”€â”€ OpenAI: í›„ë³´ ë¦¬ë­í¬/ìš”ì•½(ìƒˆ ê°€ê²Œ ìƒì„± ê¸ˆì§€) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYS = (
    "You are a strict re-ranker/summarizer. "
    "GIVEN ONLY the provided candidates from web search (with title/link/snippet), "
    "recommend top places. NEVER invent new restaurant names. "
    "Output compact JSON: {"
    '"summary":"Korean one-line with emojis", '
    '"items":[{"name":"string","reason":"Korean short","link":"url"}]'
    "} in Korean."
)

def ai_rerank(cands, topn):
    prompt = {
        "role": "user",
        "content": (
            "ì•„ë˜ëŠ” ë„¤ì´ë²„ VIEW/ë¸”ë¡œê·¸ ê²€ìƒ‰ì—ì„œ ì¶”ì¶œí•œ í›„ë³´ë“¤ì…ë‹ˆë‹¤. "
            f"ì¡°ê±´: ì„œìš¸ {gu}, í‚¤ì›Œë“œ: {food}. "
            f"ì´ í›„ë³´ë“¤ë§Œ ê°€ì§€ê³ , ìƒìœ„ {topn}ê°œë¥¼ ë½‘ì•„ ê°„ë‹¨ ì½”ë©˜íŠ¸ì™€ í•¨ê»˜ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”. "
            "ìƒˆë¡œìš´ ìƒí˜¸ëª…ì„ ë§Œë“¤ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n\n"
            + json.dumps(cands, ensure_ascii=False)
        ),
    }
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": SYS}, prompt],
        response_format={"type": "json_object"},
        temperature=0.2,
    )
    return json.loads(resp.choices[0].message.content)

# â”€â”€ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if go:
    if not food.strip():
        st.warning("ìŒì‹/í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”!")
        st.stop()

    # 1) VIEW(ë¸”ë¡œê·¸/ì¹´í˜) ìš°ì„ 
    with st.spinner("ë„¤ì´ë²„ VIEW(ë¸”ë¡œê·¸/ì¹´í˜)ì—ì„œ í›„ë³´ ìˆ˜ì§‘ ì¤‘â€¦"):
        try:
            html, query = fetch_naver_serp("view", gu, food)
            candidates = extract_candidates(html, query, max_items=30, prefer_blogs=True)
        except Exception as e:
            st.info(f"VIEW ìˆ˜ì§‘ ì‹¤íŒ¨, ì¼ë°˜ ì›¹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤: {e}")
            candidates = []

    # 2) VIEWì—ì„œ ì¶©ë¶„ì¹˜ ì•Šìœ¼ë©´ WEBë¡œ ë³´ê°•
    if not candidates:
        with st.spinner("ë„¤ì´ë²„ WEB(ì¼ë°˜)ì—ì„œ í›„ë³´ ìˆ˜ì§‘ ì¤‘â€¦"):
            try:
                html, query = fet
