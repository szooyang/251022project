 # pages/3_ë§›ì§‘_ì¶”ì²œ_ë´‡.py
import re, json, time, difflib
import requests
import pandas as pd
import streamlit as st
from urllib.parse import quote_plus

# ----------------- ê¸°ë³¸ ì„¸íŒ… -----------------
st.set_page_config(page_title="ë§›ì§‘ ì¶”ì²œ ë´‡ â€“ OpenAIë§Œ", page_icon="ğŸ”", layout="wide")
st.title("ğŸ” ë§›ì§‘ ì¶”ì²œ ë´‡ â€” OpenAI í‚¤ë§Œìœ¼ë¡œ â€˜ì‹¤ê²€ìƒ‰ ê¸°ë°˜â€™ ì¶”ì²œ")
st.caption("ë„¤ì´ë²„ ì¼ë°˜ ê²€ìƒ‰ í˜ì´ì§€ë¥¼ ê°€ë³ê²Œ íŒŒì‹±í•´ ì‹¤ì œ ê°€ê²Œë§Œ í›„ë³´ë¡œ ë§Œë“¤ê³ , OpenAIëŠ” ìš”ì•½/ë¦¬ë­í¬ë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤. (íƒ€ì‚¬ API ë¶ˆí•„ìš”)")

OPENAI_KEY = st.secrets.get("OPENAI_API_KEY", "")
if not OPENAI_KEY:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. Streamlit Secretsì— `OPENAI_API_KEY`ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

try:
    from openai import OpenAI
except Exception:
    st.error("`openai` íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. requirements.txtì— `openai>=1.30` ì¶”ê°€ í›„ ë‹¤ì‹œ ë°°í¬í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=OPENAI_KEY)

# ----------------- ì…ë ¥ UI -----------------
default_food = st.session_state.get("selected_food", "")
SEOUL_GU = [
    "ê°•ë‚¨êµ¬","ê°•ë™êµ¬","ê°•ë¶êµ¬","ê°•ì„œêµ¬","ê´€ì•…êµ¬","ê´‘ì§„êµ¬","êµ¬ë¡œêµ¬","ê¸ˆì²œêµ¬","ë…¸ì›êµ¬","ë„ë´‰êµ¬",
    "ë™ëŒ€ë¬¸êµ¬","ë™ì‘êµ¬","ë§ˆí¬êµ¬","ì„œëŒ€ë¬¸êµ¬","ì„œì´ˆêµ¬","ì„±ë™êµ¬","ì„±ë¶êµ¬","ì†¡íŒŒêµ¬","ì–‘ì²œêµ¬","ì˜ë“±í¬êµ¬",
    "ìš©ì‚°êµ¬","ì€í‰êµ¬","ì¢…ë¡œêµ¬","ì¤‘êµ¬","ì¤‘ë‘êµ¬"
]

c1,c2,c3 = st.columns([1,1,1])
with c1:
    gu = st.selectbox("ì„œìš¸ **êµ¬**", SEOUL_GU, index=SEOUL_GU.index("ê°•ë‚¨êµ¬"))
with c2:
    food = st.text_input("ìŒì‹/í‚¤ì›Œë“œ", value=str(default_food) or "ë¹„ë¹”ë°¥")
with c3:
    topk = st.slider("ìµœëŒ€ ì¶”ì²œ ìˆ˜", 3, 10, 5)

st.write("---")
go = st.button("ì‹¤ê²€ìƒ‰ìœ¼ë¡œ ì¶”ì²œ ë°›ê¸° ğŸš€", use_container_width=True)

# ----------------- ë„¤ì´ë²„ ì¼ë°˜ ê²€ìƒ‰ íŒŒì„œ -----------------
HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/130.0.0.0 Safari/537.36")
}

def fetch_naver_serp(gu: str, food: str):
    """ë„¤ì´ë²„ ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ HTML í…ìŠ¤íŠ¸(ë¬´í‚¤)"""
    q = f"{gu} {food} ì‹ë‹¹"
    url = f"https://search.naver.com/search.naver?where=web&sm=tab_jum&query={quote_plus(q)}"
    # where=web: ì¼ë°˜ ì›¹ ê²°ê³¼ ìœ„ì£¼ (place ì „ìš© DOMì´ ìì£¼ ë°”ë€Œë¯€ë¡œ ë²”ìš© íŒŒì‹±)
    r = requests.get(url, headers=HEADERS, timeout=10)
    r.raise_for_status()
    return r.text, q

def extract_candidates(html: str, q: str, max_items: int = 20):
    """
    HTMLì—ì„œ ìƒí˜¸/ë§í¬/ìŠ¤ë‹ˆí«ì„ ìœ ì—°í•˜ê²Œ ì¶”ì¶œ.
    - a href + ì¸ì ‘ í…ìŠ¤íŠ¸ë¥¼ ê¸ì–´ì„œ í›„ë³´ ìƒì„±
    - ìƒí˜¸ëª…/ì¿¼ë¦¬ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ í•„í„°ë§
    ë°˜í™˜: [{name, link, snippet, score}]
    """
    # ëª¨ë“  a íƒœê·¸ rough ì¶”ì¶œ
    links = re.findall(r'<a[^>]+href="([^"]+)"[^>]*>(.*?)</a>', html, flags=re.I|re.S)
    texts = []
    for href, inner_html in links:
        if not href.startswith("http"):
            continue
        title = re.sub("<.*?>", "", inner_html).strip()
        # ê´‘ê³ /ìŠ¤í¬ë¦½íŠ¸/ë¶ˆí•„ìš” ë§í¬ ëŒ€ëµ í•„í„°
        if not title or len(title) < 2: 
            continue
        if any(bad in href for bad in ["login", "policy", "javascript:", "naversearchad"]):
            continue
        texts.append((href, title))

    # ì œëª©ìœ¼ë¡œ 1ì°¨ í›„ë³´
    q_low = q.lower()
    cands = []
    seen = set()
    for href, title in texts:
        t = title.replace("\n"," ").strip()
        # ìœ ì‚¬ë„ ì¸¡ì •(ì¿¼ë¦¬ í‚¤ì›Œë“œ ì¼ë¶€ë§Œìœ¼ë¡œ ìŠ¤ì½”ì–´)
        sim = difflib.SequenceMatcher(None, t.lower(), q_low).ratio()
        # "ë§›ì§‘", "ì‹ë‹¹" ê°™ì€ í‚¤ì›Œë“œ/êµ¬ëª… í¬í•¨ ì—¬ë¶€ ê°€ì¤‘
        bonus = 0.05 if ("ë§›ì§‘" in t or "ì‹ë‹¹" in t) else 0.0
        score = sim + bonus
        key = (t, href)
        if key not in seen:
            seen.add(key)
            # ê°„ë‹¨í•œ ìŠ¤ë‹ˆí«: íƒ€ì´í‹€ ì£¼ë³€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€ë¡œ ë½‘ê¸°(ì—†ìœ¼ë©´ ë¹ˆê°’)
            cands.append({"name": t, "link": href, "snippet": "", "score": score})

    # ìŠ¤ë‹ˆí« ë³´ê°•(ê°„ë‹¨íˆ title í…ìŠ¤íŠ¸ ì£¼ë³€ ë¬¸ì¥ì„ ì¶”ì •)
    # ì‹¤ì œ DOM íŒŒì‹± ì—†ì´ ì •ê·œì‹ìœ¼ë¡œ ê·¼ì²˜ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ë³´ëŠ” ë¼ì´íŠ¸ ì ‘ê·¼
    for c in cands:
        # íƒ€ì´í‹€ ì¼ë¶€ê°€ htmlì— ë“¤ì–´ìˆëŠ” index ê·¼ì²˜ì—ì„œ 120ì ì •ë„ ì¶”ì¶œ ì‹œë„
        try:
            idx = html.find(c["name"])
            if idx != -1:
                start = max(0, idx - 100)
                end = min(len(html), idx + 200)
                snippet = re.sub("<.*?>", " ", html[start:end])
                snippet = re.sub(r"\s+", " ", snippet).strip()
                c["snippet"] = snippet[:160] + ("..." if len(snippet) > 160 else "")
        except Exception:
            pass

    # ì ìˆ˜ ì •ë ¬ í›„ ìƒìœ„ ë°˜í™˜
    cands.sort(key=lambda x: x["score"], reverse=True)
    return cands[:max_items]

# ----------------- OpenAI: í›„ë³´ ë¦¬ë­í¬/ìš”ì•½(ìƒˆ ê°€ê²Œ ìƒì„± ê¸ˆì§€) -----------------
SYS = (
    "You are a strict re-ranker/ summarizer. "
    "GIVEN ONLY the provided candidates from web search (with title/link/snippet), "
    "recommend top places. NEVER invent new restaurant names. "
    "Output compact JSON: {"
    '"summary":"Korean one-line with emojis", '
    '"items":[{"name":"string","reason":"Korean short","link":"url"}]'
    "} in Korean."
)

def ai_rerank(cands, topk):
    msg = {
        "role":"user",
        "content":(
            "ì•„ë˜ëŠ” ë„¤ì´ë²„ ì›¹ê²€ìƒ‰ì—ì„œ ì¶”ì¶œí•œ í›„ë³´ë“¤ì…ë‹ˆë‹¤. "
            f"ì¡°ê±´: ì„œìš¸ {gu}, í‚¤ì›Œë“œ: {food}. "
            f"ì´ í›„ë³´ë“¤ë§Œ ê°€ì§€ê³ , ìƒìœ„ {topk}ê°œë¥¼ ë½‘ì•„ ê°„ë‹¨ ì½”ë©˜íŠ¸ì™€ í•¨ê»˜ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”. "
            "ìƒˆë¡œìš´ ìƒí˜¸ëª…ì„ ë§Œë“¤ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n\n"
            + json.dumps(cands, ensure_ascii=False)
        )
    }
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role":"system","content":SYS}, msg],
        response_format={"type":"json_object"},
        temperature=0.2
    )
    return json.loads(resp.choices[0].message.content)

# ----------------- ì‹¤í–‰ -----------------
if go:
    if not food.strip():
        st.warning("ìŒì‹/í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”!")
        st.stop()

    with st.spinner("ë„¤ì´ë²„ì—ì„œ ì‹¤ì œ ê²°ê³¼ë¥¼ ê¸ì–´ì™€ í›„ë³´ ë§Œë“œëŠ” ì¤‘â€¦"):
        try:
            html, query = fetch_naver_serp(gu, food)
            candidates = extract_candidates(html, query, max_items=30)
        except Exception as e:
            st.error(f"ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            st.stop()

    if not candidates:
        st.info("í›„ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš” ğŸ¥²  í‚¤ì›Œë“œë¥¼ ë°”ê¿”ë³´ê±°ë‚˜ ì¢€ ë” ì¼ë°˜ì ì¸ í‘œí˜„ì„ ì¨ë³¼ê¹Œìš”?")
        st.stop()

    # í›„ë³´ë¥¼ OpenAIë¡œ ë¦¬ë­í¬/ìš”ì•½(ìƒˆ ê°€ê²Œ ìƒì„± ê¸ˆì§€)
    with st.spinner("AIê°€ í›„ë³´ë§Œ ê°€ì§€ê³  ì•ˆì „í•˜ê²Œ ë¦¬ë­í¬ ì¤‘â€¦"):
        try:
            ai = ai_rerank(
                [{"name": c["name"], "link": c["link"], "snippet": c["snippet"]} for c in candidates],
                topk=topk
            )
        except Exception as e:
            st.error(f"AI ìš”ì•½ ì‹¤íŒ¨: {e}")
            st.stop()

    summary = ai.get("summary","")
    items = ai.get("items", [])[:topk]

    # ê²°ê³¼ í‘œ
    st.success(f"**{gu} Â· {food}** ì‹¤ê²€ìƒ‰ ê¸°ë°˜ ì¶”ì²œ TOP{len(items)}")
    rows = []
    for it in items:
        name, reason, link = it.get("name",""), it.get("reason",""), it.get("link","#")
        # ì› í›„ë³´ì—ì„œ ê°„ë‹¨ ìŠ¤ë‹ˆí« ë§¤ì¹­
        snip = next((c["snippet"] for c in candidates if c["name"]==name and c["link"]==link), "")
        st.markdown(f"**ğŸ½ï¸ {name}**  \n- {reason}\n- ğŸ”— [ë§í¬]({link})")
        if snip:
            st.caption(snip)
        st.divider()
        rows.append({"ì´ë¦„":name, "ì½”ë©˜íŠ¸":reason, "ë§í¬":link, "ìŠ¤ë‹ˆí«":snip})

    df = pd.DataFrame(rows)
    df.index = range(1, len(df)+1)
    st.dataframe(df, use_container_width=True)

    if summary:
        st.subheader("ğŸ¤– í•œ ì¤„ ìš”ì•½")
        st.markdown(summary)

    # ë‹¤ìš´ë¡œë“œ
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ", csv, file_name="openai_only_recommendations.csv", use_container_width=True)

else:
    st.info("êµ¬/í‚¤ì›Œë“œ/ê°œìˆ˜ ì •í•˜ê³  â€˜ì‹¤ê²€ìƒ‰ìœ¼ë¡œ ì¶”ì²œ ë°›ê¸°â€™ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”. OpenAI í‚¤ë§Œìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤ ğŸ™‚")
